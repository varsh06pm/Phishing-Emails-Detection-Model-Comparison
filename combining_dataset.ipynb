{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Date\n",
      "0  Tue, 29 Aug 2000 01:26:00 -0700 (PDT)\n",
      "1  Mon, 24 Apr 2000 05:43:00 -0700 (PDT)\n",
      "2   Thu, 2 May 2002 04:54:27 -0700 (PDT)\n",
      "3   Wed, 8 Aug 2001 14:35:08 -0700 (PDT)\n",
      "4  Wed, 21 Jun 2000 04:58:00 -0700 (PDT)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset (adjust the file path as per your setup)\n",
    "df = pd.read_csv('C:/Users/pooja/OneDrive/Documents/sample_emails_labeled.csv')\n",
    "\n",
    "# Function to extract date from email content\n",
    "def extract_date(message_content):\n",
    "    lines = message_content.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if line.startswith('Date: '):\n",
    "            # Extract the date following \"Date: \" and remove leading/trailing spaces\n",
    "            date_str = line.split('Date: ', 1)[1].strip()\n",
    "            return date_str\n",
    "    return None\n",
    "\n",
    "# Apply the function to extract date and create a DataFrame with Date column\n",
    "dates_df = pd.DataFrame()\n",
    "dates_df['Date'] = df['message'].apply(extract_date)\n",
    "\n",
    "# Save to a new CSV file\n",
    "dates_df.to_csv('extracted_dates.csv', index=False)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(dates_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Date  \\\n",
      "0  Tue, 29 Aug 2000 01:26:00 -0700 (PDT)   \n",
      "1  Mon, 24 Apr 2000 05:43:00 -0700 (PDT)   \n",
      "2   Thu, 2 May 2002 04:54:27 -0700 (PDT)   \n",
      "3   Wed, 8 Aug 2001 14:35:08 -0700 (PDT)   \n",
      "4  Wed, 21 Jun 2000 04:58:00 -0700 (PDT)   \n",
      "\n",
      "                                             Message  Message_Id Spam/Ham  \n",
      "0  Message-ID: <21013688.1075844564560.JavaMail.e...           0     spam  \n",
      "1  Message-ID: <22688499.1075854130303.JavaMail.e...           1     spam  \n",
      "2  Message-ID: <27817771.1075841359502.JavaMail.e...           2     spam  \n",
      "3  Message-ID: <10695160.1075858510449.JavaMail.e...           3     spam  \n",
      "4  Message-ID: <27819143.1075853689038.JavaMail.e...           4     spam  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the original dataset (adjust the file path as per your setup)\n",
    "df = pd.read_csv('C:/Users/pooja/OneDrive/Documents/sample_emails_labeled.csv')\n",
    "\n",
    "# Function to extract date and message body from email content\n",
    "def extract_date_and_body(message_content):\n",
    "    lines = message_content.strip().split('\\n')\n",
    "    date = None\n",
    "    message_body = None\n",
    "    for line in lines:\n",
    "        if line.startswith('Date: '):\n",
    "            # Extract the date following \"Date: \" and remove leading/trailing spaces\n",
    "            date_str = line.split('Date: ', 1)[1].strip()\n",
    "            date = date_str\n",
    "        elif line.startswith('Subject: '):\n",
    "            # Skip the line starting with \"Subject: \" since it's not the start of the body\n",
    "            continue\n",
    "        elif message_body is None:\n",
    "            # Start capturing the message body once it's found\n",
    "            message_body = line.strip()\n",
    "\n",
    "    return date, message_body\n",
    "\n",
    "# Apply the function to extract date and message body, and create a DataFrame\n",
    "extracted_data = df['message'].apply(extract_date_and_body).apply(pd.Series)\n",
    "extracted_data.columns = ['Date', 'Message']\n",
    "\n",
    "# Add the 'Message_Id' column\n",
    "extracted_data['Message_Id'] = extracted_data.index\n",
    "\n",
    "# Add the 'Spam/Ham' column from the original dataset\n",
    "extracted_data['Spam/Ham'] = df['labels'].map({'Non-Important': 'spam', 'Important': 'ham'})\n",
    "\n",
    "# Save to a new CSV file\n",
    "extracted_data.to_csv('extracted_dates.csv', index=False)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(extracted_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date                                            Message  Message_Id  \\\n",
      "0  2000-08-29  Message-ID: <21013688.1075844564560.JavaMail.e...           0   \n",
      "1  2000-04-24  Message-ID: <22688499.1075854130303.JavaMail.e...           1   \n",
      "2  2002-05-02  Message-ID: <27817771.1075841359502.JavaMail.e...           2   \n",
      "3  2001-08-08  Message-ID: <10695160.1075858510449.JavaMail.e...           3   \n",
      "4  2000-06-21  Message-ID: <27819143.1075853689038.JavaMail.e...           4   \n",
      "\n",
      "  Spam/Ham  \n",
      "0     spam  \n",
      "1     spam  \n",
      "2     spam  \n",
      "3     spam  \n",
      "4     spam  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your DataFrame (replace with your actual loading method)\n",
    "df = pd.read_csv(\"C:/Users/pooja/OneDrive/Desktop/pwd_cy/extracted_dates.csv\")  # Replace with your file path\n",
    "\n",
    "# Function to convert date to standard format (YYYY-MM-DD)\n",
    "def convert_to_standard_date(date_str):\n",
    "  \"\"\"\n",
    "  This function converts a date string to a standardized format (YYYY-MM-DD).\n",
    "\n",
    "  Args:\n",
    "      date_str: The date string to be converted.\n",
    "\n",
    "  Returns:\n",
    "      str: The converted date string in YYYY-MM-DD format.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # Parse the date string (assuming various formats)\n",
    "    parsed_date = pd.to_datetime(date_str)\n",
    "    # Format the date without time (YYYY-MM-DD)\n",
    "    return parsed_date.strftime('%Y-%m-%d')\n",
    "  except ValueError:\n",
    "    # If parsing fails, return the original string\n",
    "    return date_str\n",
    "\n",
    "# Apply the function to the Date column\n",
    "df['Date'] = df['Date'].apply(convert_to_standard_date)\n",
    "\n",
    "# Save the updated DataFrame (optional)\n",
    "df.to_csv(\"your_data_updated.csv\", index=False)  # Replace with your file path\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Message ID                                            Message Spam/Ham  \\\n",
      "0           0                                                NaN      ham   \n",
      "1           1  gary , production from the high island larger ...      ham   \n",
      "2           2             - calpine daily gas nomination 1 . doc      ham   \n",
      "3           3  fyi - see note below - already done .\\nstella\\...      ham   \n",
      "4           4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham   \n",
      "\n",
      "         Date  \n",
      "0  1999-12-10  \n",
      "1  1999-12-13  \n",
      "2  1999-12-14  \n",
      "3  1999-12-14  \n",
      "4  1999-12-14  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with the actual path to your CSV file\n",
    "dataset_path = \"C:/Users/pooja/OneDrive/Documents/enron_spam_data.csv\"\n",
    "column_to_drop = \"Subject\"  # Replace with the column name you want to drop\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Drop the specified column\n",
    "data = data.drop(column_to_drop, axis=1)  # axis=1 specifies dropping a column\n",
    "\n",
    "# Save the modified data to a new CSV file (optional)\n",
    "data.to_csv(\"modified_dataset.csv\", index=False)  # Replace with desired filename\n",
    "\n",
    "# Print the first few rows to verify (optional)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load dataset 1\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dataset1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/pooja/OneDrive/Desktop/pwd_cy/complete_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load dataset 2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m dataset2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/pooja/OneDrive/Desktop/pwd_cy/your_data_updated.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:586\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset 1\n",
    "dataset1 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/complete_dataset.csv')\n",
    "\n",
    "# Load dataset 2\n",
    "dataset2 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/your_data_updated.csv')\n",
    "\n",
    "# Load dataset 3\n",
    "dataset3 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/sender_reputation_scores.csv')\n",
    "\n",
    "# Concatenate dataset 1 and dataset 3\n",
    "combined_data = pd.concat([dataset1, dataset3], axis=1)\n",
    "\n",
    "# Add dataset 2 columns\n",
    "for col in dataset2.columns:\n",
    "    combined_data[col] = dataset2[col]\n",
    "\n",
    "# Fill missing values in dataset 2 columns with random numbers from 1 to 20\n",
    "random_values = np.random.randint(1, 21, size=len(combined_data))\n",
    "for col in dataset2.columns:\n",
    "    combined_data[col].fillna(pd.Series(random_values), inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_data.to_csv('combined_dataset.csv', index=False)\n",
    "\n",
    "# Print summary information\n",
    "print(\"Combined dataset shape:\", combined_data.shape)\n",
    "print(\"Saved combined dataset to 'combined_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load dataset 1\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dataset1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/pooja/OneDrive/Desktop/pwd_cy/complete_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load dataset 2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m dataset2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/pooja/OneDrive/Desktop/pwd_cy/your_data_updated.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pooja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:586\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset 1\n",
    "dataset1 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/complete_dataset.csv')\n",
    "\n",
    "# Load dataset 2\n",
    "dataset2 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/your_data_updated.csv')\n",
    "\n",
    "# Load dataset 3\n",
    "dataset3 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/your_data_updated.csv')\n",
    "\n",
    "# Select 50 spam and 50 ham from dataset 1\n",
    "spam_data_1 = dataset1[dataset1['Spam/Ham'] == 'spam'].head(50)\n",
    "ham_data_1 = dataset1[dataset1['Spam/Ham'] == 'ham'].head(50)\n",
    "\n",
    "# Select 50 spam and 50 ham from dataset 3\n",
    "spam_data_3 = dataset3[dataset3['spam/ham'] == 'spam'].head(50)\n",
    "ham_data_3 = dataset3[dataset3['spam/ham'] == 'ham'].head(50)\n",
    "\n",
    "# Concatenate selected data from dataset 1 and dataset 3\n",
    "combined_data = pd.concat([spam_data_1, ham_data_1, spam_data_3, ham_data_3], ignore_index=True)\n",
    "\n",
    "# Merge with dataset 3 based on Message_Id\n",
    "combined_data = pd.merge(combined_data, dataset3, left_on='Message_Id', right_on='message_id', how='left')\n",
    "\n",
    "# Add dataset 2 columns\n",
    "for col in dataset2.columns:\n",
    "    combined_data[col] = dataset2[col].iloc[:len(combined_data)]  # Ensure dataset 2 has enough rows\n",
    "\n",
    "# Fill missing values in dataset 2 columns with random numbers from 1 to 20\n",
    "random_values = np.random.randint(1, 21, size=len(combined_data))\n",
    "for col in dataset2.columns:\n",
    "    combined_data[col].fillna(pd.Series(random_values), inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_data.to_csv('combined_dataset.csv', index=False)\n",
    "\n",
    "# Print summary information\n",
    "print(\"Combined dataset shape:\", combined_data.shape)\n",
    "print(\"Saved combined dataset to 'combined_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (100, 10)\n",
      "Saved combined dataset to 'combined_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset 1\n",
    "dataset1 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/modified_dataset.csv')\n",
    "\n",
    "# Load dataset 3\n",
    "dataset3 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/your_data_updated.csv')\n",
    "\n",
    "# Select 50 spam and 50 ham from dataset 1\n",
    "spam_data_1 = dataset1[dataset1['Spam/Ham'] == 'spam'].head(50)\n",
    "ham_data_1 = dataset1[dataset1['Spam/Ham'] == 'ham'].head(50)\n",
    "\n",
    "# Select 50 spam and 50 ham from dataset 3\n",
    "spam_data_3 = dataset3[dataset3['Spam/Ham'] == 'Spam'].head(50)\n",
    "ham_data_3 = dataset3[dataset3['Spam/Ham'] == 'Ham'].head(50)\n",
    "\n",
    "# Combine selected data from dataset 1 and dataset 3\n",
    "combined_data = pd.concat([spam_data_1, ham_data_1, spam_data_3, ham_data_3], ignore_index=True)\n",
    "\n",
    "# Merge dataset 3 with combined_data based on Message_Id, preserving all columns\n",
    "combined_data = pd.merge(combined_data, dataset3, on='Message_Id', how='left', suffixes=('', '_from_dataset3'))\n",
    "\n",
    "# Add dataset 2 columns (assuming dataset 2 has columns 'sender_domain' and 'sender_reputation')\n",
    "dataset2 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/sender_reputation_scores.csv')\n",
    "num_rows = len(combined_data)\n",
    "combined_data['sender_domain'] = dataset2['sender_domain'].iloc[:num_rows]\n",
    "combined_data['sender_reputation'] = dataset2['sender_reputation'].iloc[:num_rows]\n",
    "\n",
    "# Fill missing values in dataset 2 columns with random numbers from 1 to 20\n",
    "combined_data['sender_domain'].fillna(np.random.randint(1, 21), inplace=True)\n",
    "combined_data['sender_reputation'].fillna(np.random.randint(1, 21), inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_data.to_csv('combined_dataset.csv', index=False)\n",
    "\n",
    "# Print summary information\n",
    "print(\"Combined dataset shape:\", combined_data.shape)\n",
    "print(\"Saved combined dataset to 'combined_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (200, 4)\n",
      "Saved combined dataset to 'combined_dataset_1_3.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pooja\\AppData\\Local\\Temp\\ipykernel_29964\\3293611892.py:4: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset1 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/modified_dataset.csv', usecols=['Date', 'Message', 'Message_ID', 'Spam/Ham'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset 1\n",
    "dataset1 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/modified_dataset.csv', usecols=['Date', 'Message', 'Message_ID', 'Spam/Ham'])\n",
    "\n",
    "# Load dataset 3\n",
    "dataset3 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/your_data_updated.csv', usecols=['Date', 'Message', 'Message_ID', 'Spam/Ham'])\n",
    "\n",
    "# Ensure 'Spam/Ham' column values are in lowercase for both datasets\n",
    "dataset1['Spam/Ham'] = dataset1['Spam/Ham'].str.lower()\n",
    "dataset3['Spam/Ham'] = dataset3['Spam/Ham'].str.lower()\n",
    "\n",
    "# Select 50 spam and 50 ham from dataset 1\n",
    "spam_data_1 = dataset1[dataset1['Spam/Ham'] == 'spam'].head(50)\n",
    "ham_data_1 = dataset1[dataset1['Spam/Ham'] == 'ham'].head(50)\n",
    "\n",
    "# Select 50 spam and 50 ham from dataset 3\n",
    "spam_data_3 = dataset3[dataset3['Spam/Ham'] == 'spam'].head(50)\n",
    "ham_data_3 = dataset3[dataset3['Spam/Ham'] == 'ham'].head(50)\n",
    "\n",
    "# Combine selected data from dataset 1 and dataset 3 in the specified order\n",
    "combined_data = pd.concat([spam_data_1, ham_data_1, spam_data_3, ham_data_3], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_data.to_csv('combined_dataset_1_3.csv', index=False)\n",
    "\n",
    "# Print summary information\n",
    "print(\"Combined dataset shape:\", combined_data.shape)\n",
    "print(\"Saved combined dataset to 'combined_dataset_1_3.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (200, 6)\n",
      "Saved combined dataset to 'combined_dataset_1_3_with_sender.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pooja\\AppData\\Local\\Temp\\ipykernel_29964\\1112950510.py:5: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset1 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/modified_dataset.csv', usecols=['Date', 'Message', 'Message_ID', 'Spam/Ham'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset 1\n",
    "dataset1 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/modified_dataset.csv', usecols=['Date', 'Message', 'Message_ID', 'Spam/Ham'])\n",
    "\n",
    "# Load dataset 3\n",
    "dataset3 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/your_data_updated.csv', usecols=['Date', 'Message', 'Message_ID', 'Spam/Ham'])\n",
    "\n",
    "# Ensure 'Spam/Ham' column values are in lowercase for both datasets\n",
    "dataset1['Spam/Ham'] = dataset1['Spam/Ham'].str.lower()\n",
    "dataset3['Spam/Ham'] = dataset3['Spam/Ham'].str.lower()\n",
    "\n",
    "# Select 50 spam and 50 ham from dataset 1\n",
    "spam_data_1 = dataset1[dataset1['Spam/Ham'] == 'spam'].head(50)\n",
    "ham_data_1 = dataset1[dataset1['Spam/Ham'] == 'ham'].head(50)\n",
    "\n",
    "# Select 50 spam and 50 ham from dataset 3\n",
    "spam_data_3 = dataset3[dataset3['Spam/Ham'] == 'spam'].head(50)\n",
    "ham_data_3 = dataset3[dataset3['Spam/Ham'] == 'ham'].head(50)\n",
    "\n",
    "# Combine selected data from dataset 1 and dataset 3 in the specified order\n",
    "combined_data = pd.concat([spam_data_1, ham_data_1, spam_data_3, ham_data_3], ignore_index=True)\n",
    "\n",
    "# Load dataset 2 (assuming it has columns 'sender_domain' and 'sender_reputation')\n",
    "dataset2 = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/sender_reputation_scores.csv', usecols=['sender_domain', 'sender_reputation'])\n",
    "\n",
    "# Determine the number of rows in combined_data\n",
    "num_rows = len(combined_data)\n",
    "\n",
    "# Add sender_domain and sender_reputation columns from dataset2\n",
    "combined_data['sender_domain'] = dataset2['sender_domain'].iloc[:num_rows]\n",
    "combined_data['sender_reputation'] = dataset2['sender_reputation'].iloc[:num_rows]\n",
    "\n",
    "# Fill missing values in sender_domain and sender_reputation with random numbers between 1 and 20\n",
    "combined_data['sender_domain'].fillna(np.random.randint(1, 21), inplace=True)\n",
    "combined_data['sender_reputation'].fillna(np.random.randint(1, 21), inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_data.to_csv('combined_dataset_1_3_with_sender.csv', index=False)\n",
    "\n",
    "# Print summary information\n",
    "print(\"Combined dataset shape:\", combined_data.shape)\n",
    "print(\"Saved combined dataset to 'combined_dataset_1_3_with_sender.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes saved to 'combined_dataset_1_3_with_sender.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "dataset = pd.read_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/combined_dataset_1_3_with_sender.csv')\n",
    "\n",
    "# Display unique values in 'sender_domain' column\n",
    "unique_domains = dataset['sender_domain'].unique()\n",
    "\n",
    "# Create a mapping dictionary to convert each unique domain to a unique numeric code\n",
    "domain_mapping = {domain: idx + 1000 for idx, domain in enumerate(unique_domains)}\n",
    "\n",
    "# Replace non-numeric values in 'sender_domain' column with mapped numeric values\n",
    "dataset['sender_domain'] = dataset['sender_domain'].map(domain_mapping).fillna(dataset['sender_domain'])\n",
    "\n",
    "# Save the updated dataset back to the same CSV file\n",
    "dataset.to_csv('C:/Users/pooja/OneDrive/Desktop/pwd_cy/combined_dataset_1_3_with_sender.csv', index=False)\n",
    "\n",
    "print(\"Changes saved to 'combined_dataset_1_3_with_sender.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
